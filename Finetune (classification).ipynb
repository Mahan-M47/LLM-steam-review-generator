{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDWqKrwH1uw_"
   },
   "source": [
    "# Fine-tuninig the LLM Model\n",
    "Mahan Madani - Mohammad Mehdi Begmaz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOoq6QpD1uxC"
   },
   "source": [
    "## Load Dataset and important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NJXP9Phy1uxC"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "\n",
    "from pynvml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3_-hrFkP1uxD"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JG4o9Uzk1uxD",
    "outputId": "cc070ac6-44bb-4aef-85da-536bab7063ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['review', 'voted_up', 'votes_up', 'votes_funny', 'weighted_vote_score',\n",
      "       'word_count', 'profanity'],\n",
      "      dtype='object')\n",
      "(10000, 7)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./dataset/BG3_reviews_preprocessed.csv\")  # load the preprocessed version of the dataset\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GopRdoFc1uxD"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\":0, \"Positive\":1}\n",
    "\n",
    "model_name = 'gpt2'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['voted_up'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9609\n",
       "0     391\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh08ISMp1uxE"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(df[['review', 'label']])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        self.tokenizer.truncation_side = \"right\"\n",
    "\n",
    "        return self.tokenizer(\n",
    "            examples[\"review\"],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbd4668869d4482b005d53255f76000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_wrapper = TokenizerWrapper(tokenizer)\n",
    "\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenizer_wrapper.tokenize_function,\n",
    "    num_proc=4,\n",
    "    remove_columns=train_dataset.column_names.remove('label'),\n",
    "    batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# define an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "\n",
    "    # Prints the number of trainable parameters in the model.\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 148992 || all params: 124590336 || trainable%: 0.11958551905663052\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                        r=4,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.01,\n",
    "                        target_modules = ['c_attn'])\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-lora-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4116dcaf57c4d3fa6ece04b38b04bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./classification\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2924, 'learning_rate': 0.00096, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./classification\\checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2531, 'learning_rate': 0.00092, 'epoch': 0.4}\n",
      "{'loss': 0.2449, 'learning_rate': 0.00088, 'epoch': 0.6}\n",
      "{'loss': 0.1974, 'learning_rate': 0.00084, 'epoch': 0.8}\n",
      "{'loss': 0.2437, 'learning_rate': 0.0008, 'epoch': 1.0}\n",
      "{'loss': 0.2838, 'learning_rate': 0.00076, 'epoch': 1.2}\n",
      "{'loss': 0.2486, 'learning_rate': 0.0007199999999999999, 'epoch': 1.4}\n",
      "{'loss': 0.2214, 'learning_rate': 0.00068, 'epoch': 1.6}\n",
      "{'loss': 0.2209, 'learning_rate': 0.00064, 'epoch': 1.8}\n",
      "{'loss': 0.2711, 'learning_rate': 0.0006, 'epoch': 2.0}\n",
      "{'loss': 0.2224, 'learning_rate': 0.0005600000000000001, 'epoch': 2.2}\n",
      "{'loss': 0.247, 'learning_rate': 0.0005200000000000001, 'epoch': 2.4}\n",
      "{'loss': 0.224, 'learning_rate': 0.00048, 'epoch': 2.6}\n",
      "{'loss': 0.2515, 'learning_rate': 0.00044, 'epoch': 2.8}\n",
      "{'loss': 0.2584, 'learning_rate': 0.0004, 'epoch': 3.0}\n",
      "{'loss': 0.2446, 'learning_rate': 0.00035999999999999997, 'epoch': 3.2}\n",
      "{'loss': 0.2183, 'learning_rate': 0.00032, 'epoch': 3.4}\n",
      "{'loss': 0.2242, 'learning_rate': 0.00028000000000000003, 'epoch': 3.6}\n",
      "{'loss': 0.2385, 'learning_rate': 0.00024, 'epoch': 3.8}\n",
      "{'loss': 0.2756, 'learning_rate': 0.0002, 'epoch': 4.0}\n",
      "{'loss': 0.2618, 'learning_rate': 0.00016, 'epoch': 4.2}\n",
      "{'loss': 0.2177, 'learning_rate': 0.00012, 'epoch': 4.4}\n",
      "{'loss': 0.2531, 'learning_rate': 8e-05, 'epoch': 4.6}\n",
      "{'loss': 0.2025, 'learning_rate': 4e-05, 'epoch': 4.8}\n",
      "{'loss': 0.2466, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 1468.3685, 'train_samples_per_second': 34.051, 'train_steps_per_second': 8.513, 'train_loss': 0.24253839477539063, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# train model\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/classification\\\\tokenizer_config.json',\n",
       " './model/classification\\\\special_tokens_map.json',\n",
       " './model/classification\\\\vocab.json',\n",
       " './model/classification\\\\merges.txt',\n",
       " './model/classification\\\\added_tokens.json',\n",
       " './model/classification\\\\tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model parameters\n",
    "model.save_pretrained(\"./model/classification\")\n",
    "tokenizer.save_pretrained(\"./model/classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghvJS_V_1uxF"
   },
   "source": [
    "# Classify Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "import transformers\n",
    "\n",
    "logging.set_verbosity(transformers.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = AutoModelForCausalLM.from_pretrained(\"./model/v3\")\n",
    "generative_tokenizer = AutoTokenizer.from_pretrained(\"./model/v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 stars with no issues. a good story, strong characters, good choices, no problem with fights (like the original baldur's gate games i've used in the past). great replayability too!   if you're a fan of the divinity series, this game is probably good for you. the controls are well set, all of them intuitive, and the world is set in a real world environment! i don't remember my first play with this game, but i've spent a few hours through it.   still with the bugs, and some interesting twists (eg some turnbased combat which i think could make a good game), the only downside is that it is still early access (and you may get stuck sometimes), so if you are interested in early access (and still have any ideas) please look no further!  this game is well worth the price. i'm not really sure how to sum up the game's potential since we won't know much more\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "generated_text = generative_model.generate(do_sample=True, top_k=50, top_p=0.95, pad_token_id=tokenizer.pad_token_id, max_new_tokens=200)\n",
    "generated_text = generative_tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "classify(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "It was good. - Positive\n",
      "just bad, not for me. - Positive\n",
      "Better than the first one. - Positive\n",
      "This is not worth watching even once. - Positive\n",
      "This one is a pass. - Positive\n"
     ]
    }
   ],
   "source": [
    "# define list of examples\n",
    "text_list = [\"It was good.\", \"just bad, not for me.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "for text in text_list:\n",
    "    # tokenize text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    # compute logits\n",
    "    logits = model(inputs).logits\n",
    "    # convert logits to label\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df = df[df['label'] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is tough to write because honestly, act 1 and 2 make this game the game of the year. heck, i was ready to put it in my top 5 all time favorite games. i have 100 hours in this game now and just loved the story and choices and the combat.  and then act 3 happened. i started having performance issues, i started having weird glitches were my companions would lose half their bodies and couldn't fight anymore, i've had 5 different quests completely bug out on me and even a main quest bug out that i can't even complete. not only that, but the story choices of what is going on  the main bad guys and fights aren't as thought out as things were in the first 2 acts. it is very clear that act 3 was completely rushed in my opinion to get the release out the door and does not have the same care that acts 1 and 2 had.  all of that enjoyment of acts 1 and 2 to just come up against the frustrations in act 3 killed my enthusiasm for this game. i know larian will likely fix these issues as i suspect as more people start finishing the game they will see/hear this criticism  and quite possibly they already know this. but i can't in good faith recommend someone spend 80 hours in a game and then just run up against game breaking bugs that won't allow them to finish the game they way they want to. that just feels pretty bad.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "classify(negative_df['review'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
