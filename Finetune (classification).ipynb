{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDWqKrwH1uw_"
   },
   "source": [
    "# Fine-tuninig a classification model to determine review type\n",
    "Mahan Madani - Mohammad Mehdi Begmaz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOoq6QpD1uxC"
   },
   "source": [
    "## Load Dataset and important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NJXP9Phy1uxC"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "\n",
    "from pynvml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3_-hrFkP1uxD"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JG4o9Uzk1uxD",
    "outputId": "cc070ac6-44bb-4aef-85da-536bab7063ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['review', 'voted_up', 'votes_up', 'votes_funny', 'weighted_vote_score',\n",
      "       'word_count', 'profanity'],\n",
      "      dtype='object')\n",
      "(10000, 7)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./dataset/BG3_reviews_more_negative.csv\")  # load the preprocessed version of the dataset\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GopRdoFc1uxD"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\":0, \"Positive\":1}\n",
    "\n",
    "model_name = 'gpt2'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['voted_up'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5000\n",
       "1    5000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh08ISMp1uxE"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(df[['review', 'label']])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        self.tokenizer.truncation_side = \"right\"\n",
    "\n",
    "        return self.tokenizer(\n",
    "            examples[\"review\"],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1056ff147f494b678c077e426b90ff21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_wrapper = TokenizerWrapper(tokenizer)\n",
    "\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenizer_wrapper.tokenize_function,\n",
    "    num_proc=4,\n",
    "    remove_columns=train_dataset.column_names.remove('label'),\n",
    "    batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# define an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "\n",
    "    # Prints the number of trainable parameters in the model.\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 148992 || all params: 124590336 || trainable%: 0.11958551905663052\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                        r=4,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.01,\n",
    "                        target_modules = ['c_attn'])\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-lora-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91441e0b8fc4a5bb41e1770b7f8fc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./gpt2-lora-classification\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5901, 'learning_rate': 0.00096, 'epoch': 0.2}\n",
      "{'loss': 0.6402, 'learning_rate': 0.00092, 'epoch': 0.4}\n",
      "{'loss': 0.489, 'learning_rate': 0.00088, 'epoch': 0.6}\n",
      "{'loss': 0.4692, 'learning_rate': 0.00084, 'epoch': 0.8}\n",
      "{'loss': 0.462, 'learning_rate': 0.0008, 'epoch': 1.0}\n",
      "{'loss': 0.4135, 'learning_rate': 0.00076, 'epoch': 1.2}\n",
      "{'loss': 0.4177, 'learning_rate': 0.0007199999999999999, 'epoch': 1.4}\n",
      "{'loss': 0.4347, 'learning_rate': 0.00068, 'epoch': 1.6}\n",
      "{'loss': 0.4525, 'learning_rate': 0.00064, 'epoch': 1.8}\n",
      "{'loss': 0.4217, 'learning_rate': 0.0006, 'epoch': 2.0}\n",
      "{'loss': 0.341, 'learning_rate': 0.0005600000000000001, 'epoch': 2.2}\n",
      "{'loss': 0.3644, 'learning_rate': 0.0005200000000000001, 'epoch': 2.4}\n",
      "{'loss': 0.3406, 'learning_rate': 0.00048, 'epoch': 2.6}\n",
      "{'loss': 0.3325, 'learning_rate': 0.00044, 'epoch': 2.8}\n",
      "{'loss': 0.3283, 'learning_rate': 0.0004, 'epoch': 3.0}\n",
      "{'loss': 0.2729, 'learning_rate': 0.00035999999999999997, 'epoch': 3.2}\n",
      "{'loss': 0.29, 'learning_rate': 0.00032, 'epoch': 3.4}\n",
      "{'loss': 0.3011, 'learning_rate': 0.00028000000000000003, 'epoch': 3.6}\n",
      "{'loss': 0.2601, 'learning_rate': 0.00024, 'epoch': 3.8}\n",
      "{'loss': 0.2378, 'learning_rate': 0.0002, 'epoch': 4.0}\n",
      "{'loss': 0.2161, 'learning_rate': 0.00016, 'epoch': 4.2}\n",
      "{'loss': 0.2114, 'learning_rate': 0.00012, 'epoch': 4.4}\n",
      "{'loss': 0.205, 'learning_rate': 8e-05, 'epoch': 4.6}\n",
      "{'loss': 0.2095, 'learning_rate': 4e-05, 'epoch': 4.8}\n",
      "{'loss': 0.1929, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 1555.7804, 'train_samples_per_second': 32.138, 'train_steps_per_second': 8.035, 'train_loss': 0.3557681286621094, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# train model\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/classification_v2\\\\tokenizer_config.json',\n",
       " './model/classification_v2\\\\special_tokens_map.json',\n",
       " './model/classification_v2\\\\vocab.json',\n",
       " './model/classification_v2\\\\merges.txt',\n",
       " './model/classification_v2\\\\added_tokens.json',\n",
       " './model/classification_v2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model parameters\n",
    "model.save_pretrained(\"./model/classification_v2\")\n",
    "tokenizer.save_pretrained(\"./model/classification_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghvJS_V_1uxF"
   },
   "source": [
    "## Classify Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "import transformers\n",
    "\n",
    "logging.set_verbosity(transformers.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = AutoModelForCausalLM.from_pretrained(\"./model/v3\")\n",
    "generative_tokenizer = AutoTokenizer.from_pretrained(\"./model/v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 stars with no issues. a good story, strong characters, good choices, no problem with fights (like the original baldur's gate games i've used in the past). great replayability too!   if you're a fan of the divinity series, this game is probably good for you. the controls are well set, all of them intuitive, and the world is set in a real world environment! i don't remember my first play with this game, but i've spent a few hours through it.   still with the bugs, and some interesting twists (eg some turnbased combat which i think could make a good game), the only downside is that it is still early access (and you may get stuck sometimes), so if you are interested in early access (and still have any ideas) please look no further!  this game is well worth the price. i'm not really sure how to sum up the game's potential since we won't know much more\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "generated_text = generative_model.generate(do_sample=True, top_k=50, top_p=0.95, pad_token_id=tokenizer.pad_token_id, max_new_tokens=200)\n",
    "generated_text = generative_tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "classify(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "It was good. - Positive\n",
      "just bad, not for me. - Negative\n",
      "Better than the first one. - Negative\n",
      "This is not worth watching even once. - Negative\n",
      "This one is a pass. - Positive\n"
     ]
    }
   ],
   "source": [
    "# define list of examples\n",
    "text_list = [\"It was good.\", \"just bad, not for me.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "for text in text_list:\n",
    "    # tokenize text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    # compute logits\n",
    "    logits = model(inputs).logits\n",
    "    # convert logits to label\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is early access... sure. it\\'s also been in development long enough to be a full release and i don\\'t see any of the major issues, namely gameplay mechanics, being fixed.  it\\'s unintuitive.  for example, if you want all your players to jump to a thing, you have to select them all individually or they\\'ll just stand at the spot you left them.  the grouping and ungrouping of teams is horrendous and as far as i\\'ve been able to try, there is no way to select all.  it\\'s a worse version of dragon age with worse graphics and worse gameplay.  i\\'ve been having fun playing an mmo from 1994 \"the realm online\" than i have playing the 10 hours or so of this game.  there is little direction (which is ok if you consider a d&d setting), but the markers it gives you are just terrible.  another example of this is this guy who is trying to get his tieflings out of druid\\'s grove...  he says, \"meet me in the caves\" and the marker shows the caves... yet he never moves.  you take damage from nothing, controlling your characters in a turn based game shouldn\\'t be this hard.    positive point  game seems to have a decent story and it\\'s set in a setting that i enjoy.  but anyone that has paid 70 for this meh game is just kidding themselves if they give it a positive review.  \"but moldy, it\\'s in early access\".  good, let it stay there until it\\'s developed... i\\'ll be getting my money back until it\\'s a game worth playing.  edit  i\\'m going to add.  i redownloaded the game after uninstalling because i had a friend that really wanted to play multiplayer.  we never ended up doing it, but after a recent patch, it\\'s asking for another 40gb download.  on top of the 40gb download already for the game.  a game with this little content should not be needing 80 gb of storage.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_df = df[df['label'] == 0].reset_index(drop=True)\n",
    "negative_df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if you like crpgs, larian does a great job with this.  early access  unfinished and some bugs, dont listen to the haters that say this game sucks because it is unfinished, its early access.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_df = df[df['label'] == 1].reset_index(drop=True)\n",
    "positive_df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "classify(negative_df['review'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "classify(positive_df['review'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
